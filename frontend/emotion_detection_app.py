import atexit
import os
import sys
import tempfile
from collections import deque
from pathlib import Path
from time import time as current_time

import cv2
import numpy as np
import streamlit as st
from PIL import Image

sys.path.append('../backend')

try:
    from app.services.video_processing import (
        FaceDetector,
        EmotionRecognizer,
        FaceAnalysisPipeline
    )
    from app.services.video_processing import CaptureReadError, process_video_stream

    BACKEND_AVAILABLE = True
except ImportError as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞: {e}")
    BACKEND_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π EAR –∏ HeadPose (–¥–æ–ø.)
EAR_HEADPOSE_AVAILABLE = False
try:
    from app.services.video_processing import EyeAspectRatioAnalyzer, classify_attention_by_ear
    from app.services.video_processing import HeadPoseEstimator, classify_attention_state

    EAR_HEADPOSE_AVAILABLE = True
except ImportError:
    pass

APP_TITLE = "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
APP_ICON = "üé≠"
SUPPORTED_FORMATS = ['mp4', 'avi', 'mov', 'mkv', 'webm', 'wmv']
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB

# ============================================
# CSS –°–¢–ò–õ–ò
# ============================================

st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout="wide",
    initial_sidebar_state="expanded"
)


# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö CSS —Å—Ç–∏–ª–µ–π
def load_css():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–π CSS —Ñ–∞–π–ª"""
    css_file = Path(__file__).parent / "styles.css"
    if css_file.exists():
        with open(css_file, encoding='utf-8') as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    else:
        st.warning("–§–∞–π–ª —Å—Ç–∏–ª–µ–π styles.css –Ω–µ –Ω–∞–π–¥–µ–Ω")


load_css()


# ============================================
# –ö–õ–ê–°–°–´ –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –í–ò–î–ï–û
# ============================================

class EmotionDetectionProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FaceAnalysisPipeline"""

    def __init__(self):
        self.detector = None
        self.is_initialized = False
        self.current_emotions = []

    def initialize_models(self, params):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π. 
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç EAR –∏ HeadPose Estimation, –µ—Å–ª–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.
        """
        try:
            if BACKEND_AVAILABLE:
                # –î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü
                face_detector = FaceDetector()

                # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —ç–º–æ—Ü–∏–π
                emotion_recognizer = EmotionRecognizer(window_size=params.get('window_size', 15),
                                                       confidence_threshold=params.get('confidence_threshold', 0.55),
                                                       ambiguity_threshold=params.get('ambiguity_threshold', 0.15))

                # EAR –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–¥–æ–ø.)
                ear_analyzer = None
                if EAR_HEADPOSE_AVAILABLE and params.get('enable_ear', False):
                    ear_analyzer = EyeAspectRatioAnalyzer(
                        ear_threshold=params.get('ear_threshold', 0.25),
                        consec_frames=params.get('consec_frames', 3)
                    )

                # Head Pose –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–¥–æ–ø.)
                head_pose_estimator = None
                if EAR_HEADPOSE_AVAILABLE and params.get('enable_head_pose', False):
                    head_pose_estimator = HeadPoseEstimator()

                # –û—Å–Ω–æ–≤–Ω–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
                self.detector = FaceAnalysisPipeline(
                    face_detector,
                    emotion_recognizer,
                    ear_analyzer=ear_analyzer,
                    head_pose_estimator=head_pose_estimator,
                    use_face_mesh=(ear_analyzer is not None or head_pose_estimator is not None)
                )

                self.is_initialized = True
                return True, "–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã"
            else:
                return False, "–ú–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"

        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}"

    def process_frame(self, frame, flip_h=False):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–¥—Ä –≤–∏–¥–µ–æ"""
        if not self.is_initialized or self.detector is None:
            return frame, []

        try:
            # –û—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if flip_h:
                frame = cv2.flip(frame, 1)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ —Å –ø–æ–º–æ—â—å—é –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
            analysis_result = self.detector.analyze(frame)

            processed_frame = analysis_result.image
            results = []
            for metric in analysis_result.metrics:
                result_dict = {
                    'emotion': metric.emotion,
                    'confidence': metric.confidence,
                    'bbox': metric.bbox,
                }
                if metric.ear:
                    result_dict['ear'] = {
                        'avg_ear': metric.ear.avg_ear,
                        'eyes_open': not metric.ear.is_blinking,
                        'blink_count': metric.ear.blink_count
                    }
                if metric.head_pose:
                    result_dict['head_pose'] = {
                        'pitch': metric.head_pose.pitch,
                        'yaw': metric.head_pose.yaw,
                        'roll': metric.head_pose.roll,
                        'attention_state': getattr(metric.head_pose, 'attention_state', None)
                    }
                results.append(result_dict)

            self.current_emotions = results
            return processed_frame, results

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {e}")
            return frame, []

    def get_emotion_statistics(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —ç–º–æ—Ü–∏—è–º"""
        if not self.current_emotions:
            return {}

        stats = {}
        for emotion, confidence in self.current_emotions:
            if emotion in stats:
                stats[emotion] += 1
            else:
                stats[emotion] = 1

        return stats

    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.current_emotions = []
        if hasattr(self.detector, 'face_detector'):
            if hasattr(self.detector.face_detector, 'close'):
                self.detector.face_detector.close()
        if hasattr(self.detector, 'emotion_recognizer'):
            if hasattr(self.detector.emotion_recognizer, 'reset'):
                self.detector.emotion_recognizer.reset()


class VideoFileProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤"""

    def __init__(self):
        self.detection_processor = EmotionDetectionProcessor()

    def extract_video_info(self, video_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ"""
        try:
            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                return {"error": "Cannot open video file"}

            info = {
                "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                "fps": int(cap.get(cv2.CAP_PROP_FPS)),
                "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
                "duration": round(cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS), 2) if cap.get(
                    cv2.CAP_PROP_FPS) > 0 else 0,
                "format": self._get_video_format(video_path)
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–≤—å—é
            ret, frame = cap.read()
            if ret:
                preview_path = "temp_preview.jpg"
                cv2.imwrite(preview_path, frame)
                info["preview"] = preview_path

            cap.release()
            return info

        except Exception as e:
            return {"error": f"Cannot extract video info: {str(e)}"}

    def _get_video_format(self, video_path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç –≤–∏–¥–µ–æ"""
        ext = os.path.splitext(video_path)[1].lower()
        formats = {
            '.mp4': 'MP4',
            '.avi': 'AVI',
            '.mov': 'MOV',
            '.mkv': 'MKV',
            '.webm': 'WebM',
            '.wmv': 'WMV'
        }
        return formats.get(ext, 'Unknown')

    def process_video_file(self, input_path, output_path, params, progress_callback=None):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ—Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —ç–º–æ—Ü–∏–π

        Args:
            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        Returns:
            (success, message, output_path, statistics)
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
            success, message = self.detection_processor.initialize_models(params)
            if not success:
                return False, message, None, {}

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                return False, f"Cannot open video file: {input_path}", None, {}

            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            if fps == 0:
                fps = 30

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # –°–æ–∑–¥–∞–µ–º VideoWriter –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            all_emotions = []
            frame_count = 0

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º  –∫–∞–¥—Ä
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä
                processed_frame, emotions = self.detection_processor.process_frame(
                    frame,
                    flip_h=params.get('flip_h', False)
                )

                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä
                out.write(processed_frame)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                for result in emotions:
                    all_emotions.append(result['emotion'])

                frame_count += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if progress_callback and total_frames > 0:
                    progress = frame_count / total_frames
                    progress_callback(progress, frame_count, total_frames, emotions)

            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
            cap.release()
            out.release()

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            statistics = self._calculate_statistics(all_emotions, frame_count)

            return True, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ", output_path, statistics

        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ: {str(e)}", None, {}

    def _calculate_statistics(self, all_emotions, total_frames):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —ç–º–æ—Ü–∏—è–º"""
        if not all_emotions:
            return {}

        stats = {}
        for emotion in all_emotions:
            if emotion in stats:
                stats[emotion] += 1
            else:
                stats[emotion] = 1

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        total_detections = len(all_emotions)
        if total_detections > 0:
            for emotion in stats:
                stats[f"{emotion}_percent"] = (stats[emotion] / total_detections) * 100

        stats['total_frames'] = total_frames
        stats['total_detections'] = total_detections
        stats['detection_rate'] = (total_detections / total_frames) * 100 if total_frames > 0 else 0

        return stats

    def extract_sample_frames(self, video_path, num_frames=4):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–µ–≤—å—é"""
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if total_frames == 0:
                return []

            # –í—ã–±–∏—Ä–∞–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã
            frame_indices = np.linspace(0, total_frames - 1, min(num_frames, total_frames), dtype=int)

            frames = []
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(frame_rgb)

            cap.release()
            return frames

        except Exception as e:
            return []


# ============================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–°–°–ò–ò
# ============================================

if 'video_processor' not in st.session_state:
    st.session_state.video_processor = VideoFileProcessor()

if 'detection_processor' not in st.session_state:
    st.session_state.detection_processor = EmotionDetectionProcessor()

if 'uploaded_file_path' not in st.session_state:
    st.session_state.uploaded_file_path = None

if 'processing_status' not in st.session_state:
    st.session_state.processing_status = "idle"

if 'result_path' not in st.session_state:
    st.session_state.result_path = None

if 'video_info' not in st.session_state:
    st.session_state.video_info = {}

if 'processing_progress' not in st.session_state:
    st.session_state.processing_progress = 0

if 'current_emotions' not in st.session_state:
    st.session_state.current_emotions = []

if 'emotion_statistics' not in st.session_state:
    st.session_state.emotion_statistics = {}

if 'backend_params' not in st.session_state:
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    st.session_state.backend_params = {
        'min_detection_confidence': 0.5,
        'window_size': 15,
        'confidence_threshold': 0.55,
        'ambiguity_threshold': 0.15,
        'margin': 20,
        'flip_h': False,
        'show_preview': False,
        'enable_ear': False,  # –í–∫–ª—é—á–∏—Ç—å EAR –∞–Ω–∞–ª–∏–∑
        'enable_head_pose': False,  # –í–∫–ª—é—á–∏—Ç—å Head Pose –∞–Ω–∞–ª–∏–∑
        'ear_threshold': 0.25,  # –ü–æ—Ä–æ–≥ EAR –¥–ª—è –∑–∞–∫—Ä—ã—Ç—ã—Ö –≥–ª–∞–∑
        'consec_frames': 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è –º–æ—Ä–≥–∞–Ω–∏—è (1 = –ª—é–±–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –≥–ª–∞–∑)
    }

if 'webcam_running' not in st.session_state:
    st.session_state.webcam_running = False

if 'webcam_detector' not in st.session_state:
    st.session_state.webcam_detector = None

if 'prev_webcam_params' not in st.session_state:
    st.session_state.prev_webcam_params = None

if 'vis_params' not in st.session_state:
    st.session_state.vis_params = {
        'show_fps': True,        # FPS-—Å—á—ë—Ç—á–∏–∫ –Ω–∞ –∫–∞–¥—Ä–µ
        'show_emotions': True,   # –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ —Å —ç–º–æ—Ü–∏—è–º–∏ –ø–æ–¥ –≤–∏–¥–µ–æ
        'show_ear_info': True,   # EAR-–¥–∞–Ω–Ω—ã–µ –≤ –±–ª–æ–∫–µ —ç–º–æ—Ü–∏–π
        'show_hpe_info': True,   # HPE-–¥–∞–Ω–Ω—ã–µ –≤ –±–ª–æ–∫–µ —ç–º–æ—Ü–∏–π
    }


# ============================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================

def display_header():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown(f'<h1 class="main-header">{APP_TITLE}</h1>', unsafe_allow_html=True)
        st.markdown(
            '<p style="text-align: center; color: #666; font-size: 1.2rem; margin-bottom: 2rem;">–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ–±-–∫–∞–º–µ—Ä—É –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏</p>',
            unsafe_allow_html=True)


def display_sidebar():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    with st.sidebar:
        st.markdown("### üé≠ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        st.markdown("#### ‚ÑπÔ∏è –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã")

        if BACKEND_AVAILABLE:
            st.success("‚úÖ –ú–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω")
        else:
            st.error("‚ùå –ú–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            st.info("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª face_detection.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")

        st.markdown("---")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        st.markdown("#### ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        # Face Detector Parameters
        st.markdown("##### –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü")
        st.session_state.backend_params['min_detection_confidence'] = st.slider(
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏ (min_detection_confidence)",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['min_detection_confidence'],
            step=0.01,
            help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü"
        )

        # Emotion Recognizer Parameters
        st.markdown("##### –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π")

        st.session_state.backend_params['window_size'] = st.slider(
            "–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (window_size)",
            min_value=3,
            max_value=30,
            value=st.session_state.backend_params['window_size'],
            step=1,
            help="–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è temporal smoothing (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –∫–∞–¥—Ä–æ–≤)"
        )

        st.session_state.backend_params['confidence_threshold'] = st.slider(
            "–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (confidence_threshold)",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['confidence_threshold'],
            step=0.01,
            help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —ç–º–æ—Ü–∏–∏ (–Ω–∏–∂–µ ‚Üí fallback –∫ Neutral)"
        )

        st.session_state.backend_params['ambiguity_threshold'] = st.slider(
            "–ü–æ—Ä–æ–≥ –∞–º–±–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (ambiguity_threshold)",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.backend_params['ambiguity_threshold'],
            step=0.01,
            help="–ü–æ—Ä–æ–≥ –¥–ª—è –∞–º–±–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö —ç–º–æ—Ü–∏–π (–µ—Å–ª–∏ –¥–≤–µ —ç–º–æ—Ü–∏–∏ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–∏ ‚Üí Neutral)"
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ EAR –∏ HeadPose
        if EAR_HEADPOSE_AVAILABLE:
            st.markdown("##### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏")

            st.session_state.backend_params['enable_ear'] = st.checkbox(
                "–í–∫–ª—é—á–∏—Ç—å Eye Aspect Ratio (enable_ear)",
                value=st.session_state.backend_params['enable_ear'],
                help="–ê–Ω–∞–ª–∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥–ª–∞–∑ –∏ –º–æ—Ä–≥–∞–Ω–∏—è (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)"
            )

            if st.session_state.backend_params['enable_ear']:
                st.session_state.backend_params['ear_threshold'] = st.slider(
                    "–ü–æ—Ä–æ–≥ EAR (ear_threshold)",
                    min_value=0.10,
                    max_value=0.40,
                    value=st.session_state.backend_params['ear_threshold'],
                    step=0.01,
                    help="–ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–∫—Ä—ã—Ç—ã—Ö –≥–ª–∞–∑ (–º–µ–Ω—å—à–µ ‚Üí —Å—Ç—Ä–æ–∂–µ)"
                )

                st.session_state.backend_params['consec_frames'] = st.slider(
                    "–ö–∞–¥—Ä–æ–≤ –¥–ª—è –º–æ—Ä–≥–∞–Ω–∏—è (consec_frames)",
                    min_value=1,
                    max_value=10,
                    value=st.session_state.backend_params['consec_frames'],
                    step=1,
                    help="–ú–∏–Ω–∏–º—É–º –∫–∞–¥—Ä–æ–≤ —Å –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –≥–ª–∞–∑–∞–º–∏ –¥–ª—è –∑–∞—Å—á–∏—Ç—ã–≤–∞–Ω–∏—è –º–æ—Ä–≥–∞–Ω–∏—è"
                )

            st.session_state.backend_params['enable_head_pose'] = st.checkbox(
                "–í–∫–ª—é—á–∏—Ç—å Head Pose Estimation (enable_head_pose)",
                value=st.session_state.backend_params['enable_head_pose'],
                help="–û—Ü–µ–Ω–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ –∏ –ø–æ–∑—ã –≥–æ–ª–æ–≤—ã"
            )

        # –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        st.markdown("##### –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.session_state.backend_params['flip_h'] = st.checkbox(
            "–û—Ç—Ä–∞–∑–∏—Ç—å –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏ (flip_h)",
            value=st.session_state.backend_params['flip_h'],
            help="–û—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏ (–¥–ª—è –≤–µ–±-–∫–∞–º–µ—Ä—ã)"
        )

        st.session_state.backend_params['show_preview'] = st.checkbox(
            "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–≤—å—é (show_preview)",
            value=st.session_state.backend_params['show_preview'],
            help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–≤—å—é –∫–∞–¥—Ä–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        st.markdown("##### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")

        st.session_state.vis_params['show_fps'] = st.checkbox(
            "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å FPS",
            value=st.session_state.vis_params['show_fps'],
            help="–°—á—ë—Ç—á–∏–∫ FPS –Ω–∞ –∫–∞–¥—Ä–µ"
        )

        st.session_state.vis_params['show_emotions'] = st.checkbox(
            "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏",
            value=st.session_state.vis_params['show_emotions'],
            help="–ë–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ–± —ç–º–æ—Ü–∏—è—Ö –ø–æ–¥ –≤–∏–¥–µ–æ"
        )

        if EAR_HEADPOSE_AVAILABLE:
            st.session_state.vis_params['show_ear_info'] = st.checkbox(
                "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ EAR",
                value=st.session_state.vis_params['show_ear_info'],
                help="EAR-–¥–∞–Ω–Ω—ã–µ –≤ –±–ª–æ–∫–µ —ç–º–æ—Ü–∏–π (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ EAR –≤–∫–ª—é—á—ë–Ω)"
            )

            st.session_state.vis_params['show_hpe_info'] = st.checkbox(
                "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ HPE",
                value=st.session_state.vis_params['show_hpe_info'],
                help="–î–∞–Ω–Ω—ã–µ Head Pose Estimation –≤ –±–ª–æ–∫–µ —ç–º–æ—Ü–∏–π (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ HPE –≤–∫–ª—é—á—ë–Ω)"
            )

        st.markdown("---")

        # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é", width='stretch'):
            st.session_state.backend_params = {
                'min_detection_confidence': 0.5,
                'window_size': 15,
                'confidence_threshold': 0.55,
                'ambiguity_threshold': 0.15,
                'flip_h': False,
                'show_preview': False
            }
            st.rerun()


def create_upload_section():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ–∫—Ü–∏—é –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞"""
    st.markdown("### üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ")

    # –ó–æ–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∏
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª",
        type=SUPPORTED_FORMATS,
        help=f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(SUPPORTED_FORMATS).upper()}",
        label_visibility="collapsed"
    )

    if uploaded_file is not None:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if uploaded_file.size > MAX_FILE_SIZE:
            st.error(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π! –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {MAX_FILE_SIZE // (1024 * 1024)}–ú–ë")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, uploaded_file.name)

        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.session_state.uploaded_file_path = temp_path

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ
        video_info = st.session_state.video_processor.extract_video_info(temp_path)
        st.session_state.video_info = video_info

        if "error" not in video_info:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            display_file_info(uploaded_file, video_info)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –≤–∏–¥–µ–æ
            display_video_preview(temp_path, video_info)

            # –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üöÄ –ù–∞—á–∞—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π", type="primary", width='stretch'):
                    if not BACKEND_AVAILABLE:
                        st.error(
                            "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É: Backend –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª face_detection.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏.")
                    else:
                        st.session_state.processing_status = "starting"
                        st.rerun()
            with col2:
                if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —Ñ–∞–π–ª", width='stretch'):
                    st.session_state.uploaded_file_path = None
                    st.rerun()
        else:
            st.error(f"–û—à–∏–±–∫–∞: {video_info['error']}")

    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª, –¥–ª—è —ç—Ç–æ–≥–æ –≤—ã–±–µ—Ä–∏—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ **Browse Files** –∏–ª–∏ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç–µ –≤ –æ–±–ª–∞—Å—Ç—å –≤—ã—à–µ.")
        st.markdown("""
        **–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:** MP4, AVI, MOV, MKV, WebM, WMV
                    
        **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä:** 100MB
        """)


def display_file_info(uploaded_file, video_info):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ"""
    st.markdown("### üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–¥–µ–æ")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            label="–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ",
            value=f"{video_info['width']}√ó{video_info['height']}"
        )

    with col2:
        duration = video_info["duration"]
        if duration >= 60:
            minutes = int(duration // 60)
            seconds = int(duration % 60)
            duration_str = f"{minutes}:{seconds:02d}"
        else:
            duration_str = f"{duration:.1f}—Å"

        st.metric(
            label="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
            value=duration_str
        )

    with col3:
        st.metric(
            label="FPS",
            value=video_info["fps"]
        )

    with col4:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        st.metric(
            label="–†–∞–∑–º–µ—Ä (–ú–ë)",
            value=f"{file_size_mb:.1f}"
        )


def display_video_preview(video_path, video_info):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ø—Ä–µ–≤—å—é –≤–∏–¥–µ–æ"""
    st.markdown("### üëÄ –ü—Ä–µ–≤—å—é –≤–∏–¥–µ–æ")

    # –û—Å–Ω–æ–≤–Ω–æ–µ –≤–∏–¥–µ–æ
    st.markdown('<div class="video-container">', unsafe_allow_html=True)
    video_bytes = open(video_path, "rb").read()
    st.video(video_bytes)
    st.markdown('</div>', unsafe_allow_html=True)

    # –ü—Ä–∏–º–µ—Ä—ã –∫–∞–¥—Ä–æ–≤
    st.markdown("#### üì∏ –ü—Ä–∏–º–µ—Ä—ã –∫–∞–¥—Ä–æ–≤")

    frames = st.session_state.video_processor.extract_sample_frames(video_path, 4)
    if frames:
        cols = st.columns(4)
        for idx, (col, frame) in enumerate(zip(cols, frames)):
            with col:
                img = Image.fromarray(frame)
                st.image(img, caption=f"–ö–∞–¥—Ä {idx + 1}", width='stretch')


def process_video():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ"""
    if st.session_state.processing_status == "starting" and st.session_state.uploaded_file_path:
        st.session_state.processing_status = "processing"

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–Ω–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        st.markdown('<div class="processing-card">', unsafe_allow_html=True)
        st.markdown("### ‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞—à–µ–≥–æ –≤–∏–¥–µ–æ")

        progress_bar = st.progress(0)
        status_text = st.empty()
        stats_text = st.empty()

        # –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        def update_progress(progress, current_frame, total_frames, emotions):
            progress_bar.progress(progress)
            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ {current_frame} –∏–∑ {total_frames} ({progress * 100:.1f}%)")

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if emotions:
                emotion_stats = {}
                for result in emotions:
                    emotion = result['emotion']
                    emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1

                stats_text.markdown("**–¢–µ–∫—É—â–∏–µ —ç–º–æ—Ü–∏–∏:** " + ", ".join([f"{k}: {v}" for k, v in emotion_stats.items()]))

        # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        input_path = st.session_state.uploaded_file_path
        input_name = os.path.splitext(os.path.basename(input_path))[0]
        timestamp = int(current_time())
        output_filename = f"emotion_detected_{input_name}_{timestamp}.mp4"
        output_path = os.path.join(tempfile.gettempdir(), output_filename)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        success, message, result_path, statistics = st.session_state.video_processor.process_video_file(
            input_path,
            output_path,
            st.session_state.backend_params,
            update_progress
        )

        if success:
            st.session_state.processing_status = "completed"
            st.session_state.result_path = result_path
            st.session_state.emotion_statistics = statistics
        else:
            st.session_state.processing_status = "failed"
            st.session_state.error_message = message

        st.markdown('</div>', unsafe_allow_html=True)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if st.session_state.processing_status == "completed":
            display_result()
        elif st.session_state.processing_status == "failed":
            display_error()


def display_result():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    st.markdown('<div class="result-card">', unsafe_allow_html=True)
    st.markdown("### ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    result_path = st.session_state.result_path

    if result_path and os.path.exists(result_path):
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
        st.markdown("#### üé¨ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ")
        st.markdown('<div class="video-container">', unsafe_allow_html=True)
        st.video(result_path)
        st.markdown('</div>', unsafe_allow_html=True)

        # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–æ—Ü–∏–π
        if st.session_state.emotion_statistics:
            st.markdown("#### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–æ—Ü–∏–π")

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —ç–º–æ—Ü–∏–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è)
            emotion_stats = {k: v for k, v in st.session_state.emotion_statistics.items()
                             if not k.endswith('_percent') and k not in ['total_frames', 'total_detections',
                                                                         'detection_rate']}

            if emotion_stats:
                cols = st.columns(len(emotion_stats))
                for idx, (emotion, count) in enumerate(emotion_stats.items()):
                    with cols[idx % len(cols)]:
                        percent_key = f"{emotion}_percent"
                        percent = st.session_state.emotion_statistics.get(percent_key, 0)
                        st.metric(emotion.capitalize(), f"{count} ({percent:.1f}%)")

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            st.markdown("#### üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤", st.session_state.emotion_statistics.get('total_frames', 0))
            with col2:
                st.metric("–î–µ—Ç–µ–∫—Ü–∏–π –ª–∏—Ü", st.session_state.emotion_statistics.get('total_detections', 0))
            with col3:
                st.metric("–ü—Ä–æ—Ü–µ–Ω—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏", f"{st.session_state.emotion_statistics.get('detection_rate', 0):.1f}%")

        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        with open(result_path, "rb") as f:
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ",
                data=f,
                file_name=os.path.basename(result_path),
                mime="video/mp4",
                type="primary",
                width='stretch'
            )

    else:
        st.warning("–§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥—Ä—É–≥–æ–µ –≤–∏–¥–µ–æ", width='stretch'):
            st.session_state.uploaded_file_path = None
            st.session_state.processing_status = "idle"
            st.session_state.result_path = None
            st.session_state.emotion_statistics = {}
            st.rerun()

    st.markdown('</div>', unsafe_allow_html=True)


def display_error():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –æ—à–∏–±–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    st.markdown('<div class="error-card">', unsafe_allow_html=True)
    st.markdown("### ‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å")

    error_msg = getattr(st.session_state, 'error_message', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
    st.error(f"–û—à–∏–±–∫–∞: {error_msg}")

    # –°–æ–≤–µ—Ç—ã –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –Ω–µ–ø–æ–ª–∞–¥–æ–∫
    st.markdown("#### üîß –°–æ–≤–µ—Ç—ã –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –Ω–µ–ø–æ–ª–∞–¥–æ–∫:")
    st.markdown("""
    1. ‚úÖ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª `face_detection.py` –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    2. ‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
    3. ‚úÖ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤–∏–¥–µ–æ (–º–µ–Ω–µ–µ 1 –º–∏–Ω—É—Ç—ã)
    4. ‚úÖ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–æ—Ä–º–∞—Ç –≤–∏–¥–µ–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    5. ‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
    """)

    if st.button("üîÑ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞", width='stretch'):
        st.session_state.processing_status = "idle"
        st.rerun()

    st.markdown('</div>', unsafe_allow_html=True)


def create_webcam_section():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ–∫—Ü–∏—é —Ä–∞–±–æ—Ç—ã —Å –≤–µ–±-–∫–∞–º–µ—Ä–æ–π"""
    st.markdown("### üì∑ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π —á–µ—Ä–µ–∑ –≤–µ–±-–∫–∞–º–µ—Ä—É")

    if not BACKEND_AVAILABLE:
        st.warning(
            "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π —á–µ—Ä–µ–∑ –≤–µ–±-–∫–∞–º–µ—Ä—É —Ç—Ä–µ–±—É–µ—Ç –º–æ–¥—É–ª—å –±—ç–∫–µ–Ω–¥–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª face_detection.py –¥–æ—Å—Ç—É–ø–µ–Ω.")
        return

    col1, col2 = st.columns([3, 1])

    with col1:
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        control_col1, control_col2 = st.columns(2)

        with control_col1:
            start_webcam = st.button("üé¨ –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É", type="primary", width='stretch')

        with control_col2:
            stop_webcam = st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É", type="secondary", width='stretch')

        # –ú–µ—Å—Ç–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ
        webcam_placeholder = st.empty()
        emotions_placeholder = st.empty()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_placeholder = st.empty()
        fps_placeholder = st.empty()

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤–µ–±-–∫–∞–º–µ—Ä—ã
        if start_webcam:
            st.session_state.webcam_running = True

        if stop_webcam:
            st.session_state.webcam_running = False

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-–∫–∞–º–µ—Ä—É
        if st.session_state.get('webcam_running', False):
            cap = cv2.VideoCapture(0)

            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É")
                st.session_state.webcam_running = False
            else:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–º–µ—Ä—ã
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if st.session_state.webcam_detector is None:
                    params = st.session_state.backend_params

                    face_detector = FaceDetector()
                    emotion_recognizer = EmotionRecognizer(window_size=params['window_size'],
                                                           confidence_threshold=params['confidence_threshold'],
                                                           ambiguity_threshold=params['ambiguity_threshold'])

                    # EAR –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–¥–æ–ø.)
                    ear_analyzer = None
                    if EAR_HEADPOSE_AVAILABLE and params.get('enable_ear', False):
                        ear_analyzer = EyeAspectRatioAnalyzer(
                            ear_threshold=params.get('ear_threshold', 0.25),
                            consec_frames=params.get('consec_frames', 3)
                        )

                    # Head Pose –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–¥–æ–ø.)
                    head_pose_estimator = None
                    if EAR_HEADPOSE_AVAILABLE and params.get('enable_head_pose', False):
                        head_pose_estimator = HeadPoseEstimator()

                    st.session_state.webcam_detector = FaceAnalysisPipeline(
                        face_detector,
                        emotion_recognizer,
                        ear_analyzer=ear_analyzer,
                        head_pose_estimator=head_pose_estimator,
                        use_face_mesh=(ear_analyzer is not None or head_pose_estimator is not None)
                    )
                    st.session_state.prev_webcam_params = params.copy()

                # Hot-reload: –ø—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –î–û —Ü–∏–∫–ª–∞ (–ø—Ä–∏ –∫–∞–∂–¥–æ–º rerune —Å–∫—Ä–∏–ø—Ç–∞)
                elif st.session_state.prev_webcam_params != st.session_state.backend_params:
                    detector = st.session_state.webcam_detector
                    params = st.session_state.backend_params
                    prev_params = st.session_state.prev_webcam_params or {}

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ FaceDetector
                    if params['min_detection_confidence'] != prev_params.get('min_detection_confidence'):
                        detector.face_detector.set_min_detection_confidence(params['min_detection_confidence'])

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ EmotionRecognizer
                    if params['window_size'] != prev_params.get('window_size'):
                        detector.emotion_recognizer.set_window_size(params['window_size'])

                    if params['confidence_threshold'] != prev_params.get('confidence_threshold'):
                        detector.emotion_recognizer.set_confidence_threshold(params['confidence_threshold'])

                    if params['ambiguity_threshold'] != prev_params.get('ambiguity_threshold'):
                        detector.emotion_recognizer.set_ambiguity_threshold(params['ambiguity_threshold'])

                    # Hot-reload EAR –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                    if EAR_HEADPOSE_AVAILABLE:
                        # –í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ EAR
                        if params.get('enable_ear') != prev_params.get('enable_ear'):
                            if params.get('enable_ear'):
                                new_ear = EyeAspectRatioAnalyzer(
                                    ear_threshold=params.get('ear_threshold', 0.25),
                                    consec_frames=params.get('consec_frames', 1)
                                )
                                detector.set_ear_analyzer(new_ear)
                            else:
                                detector.set_ear_analyzer(None)
                        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ EAR (–±–µ–∑ —Å–±—Ä–æ—Å–∞ —Å—á—ë—Ç—á–∏–∫–æ–≤)
                        elif params.get('enable_ear') and detector.ear_analyzer:
                            if params.get('ear_threshold') != prev_params.get('ear_threshold'):
                                detector.ear_analyzer.set_ear_threshold(params.get('ear_threshold', 0.25))
                            if params.get('consec_frames') != prev_params.get('consec_frames'):
                                detector.ear_analyzer.set_consec_frames(params.get('consec_frames', 1))

                        # Hot-reload HeadPose –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        if params.get('enable_head_pose') != prev_params.get('enable_head_pose'):
                            if params.get('enable_head_pose'):
                                detector.set_head_pose_estimator(HeadPoseEstimator())
                            else:
                                detector.set_head_pose_estimator(None)

                    st.session_state.prev_webcam_params = params.copy()

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é process_video_stream –∏–∑ –±—ç–∫–µ–Ω–¥–∞
                try:
                    fps_history = deque(maxlen=3)
                    for _ in range(3):
                        fps_history.append(0.0)

                    emotion_history = []
                    start_time = current_time()
                    vis = st.session_state.vis_params

                    for img, emotions in process_video_stream(cap, st.session_state.webcam_detector,
                                                              flip_h=st.session_state.backend_params['flip_h']):
                        if not st.session_state.get('webcam_running', False):
                            break

                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        if emotions:
                            for result in emotions:
                                emotion_history.append(result['emotion'])

                        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
                        if len(emotion_history) > 100:
                            emotion_history = emotion_history[-100:]

                        # –†–∞—Å—á–µ—Ç FPS
                        fps = 1 / (current_time() - start_time)
                        fps_history.append(fps)
                        avg_fps = round(sum(fps_history) / len(fps_history))

                        # FPS –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
                        if vis.get('show_fps', True):
                            cv2.putText(img, f'FPS: {avg_fps}', (5, 20),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)

                        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit
                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–¥—Ä–∞
                        webcam_placeholder.image(img_rgb, channels="RGB", width='stretch')

                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —ç–º–æ—Ü–∏–π –∏ –¥–æ–ø. –¥–∞–Ω–Ω—ã—Ö
                        if emotions and vis.get('show_emotions', True):
                            emotion_text = "**–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ª–∏—Ü–∞:**\n\n"
                            for i, result in enumerate(emotions):
                                emotion_text += f"**–õ–∏—Ü–æ {i + 1}:**\n"
                                emotion_text += f"- –≠–º–æ—Ü–∏—è: {result['emotion']} ({result['confidence']:.2f})\n"

                                # EAR –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
                                if result.get('ear') and vis.get('show_ear_info', True):
                                    ear_data = result['ear']
                                    emotion_text += f"- EAR: {ear_data['avg_ear']:.3f} "
                                    emotion_text += f"({'–û—Ç–∫—Ä—ã—Ç—ã' if ear_data['eyes_open'] else '–ó–∞–∫—Ä—ã—Ç—ã'}) "
                                    emotion_text += f"[–ú–æ—Ä–≥–∞–Ω–∏—è: {ear_data['blink_count']}]\n"

                                # HeadPose –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
                                if result.get('head_pose') and vis.get('show_hpe_info', True):
                                    hp = result['head_pose']
                                    emotion_text += f"- –ü–æ–∑–∞: Pitch={hp['pitch']:.0f}¬∞ Yaw={hp['yaw']:.0f}¬∞ Roll={hp['roll']:.0f}¬∞\n"
                                    if 'attention_state' in hp:
                                        emotion_text += f"- –°–æ—Å—Ç–æ—è–Ω–∏–µ: {hp['attention_state']}\n"

                                emotion_text += "\n"

                            emotions_placeholder.markdown(emotion_text)
                        else:
                            emotions_placeholder.empty()

                        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        if emotion_history:
                            stats = {}
                            for emotion in emotion_history:
                                stats[emotion] = stats.get(emotion, 0) + 1

                            stats_text = "**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–æ—Ü–∏–π:**\n"
                            for emotion, count in stats.items():
                                percent = (count / len(emotion_history)) * 100
                                stats_text += f"{emotion}: {percent:.1f}%\n"

                            stats_placeholder.markdown(stats_text)

                        fps_placeholder.metric("–¢–µ–∫—É—â–∏–π FPS", avg_fps)
                        start_time = current_time()

                except CaptureReadError as e:
                    st.error(f"–û—à–∏–±–∫–∞ –≤–µ–±-–∫–∞–º–µ—Ä—ã: {e}")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-–∫–∞–º–µ—Ä—ã: {e}")
                finally:
                    cap.release()

                    # Cleanup —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —è–≤–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–µ (webcam_running = False)
                    # –ü—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏ Streamlit –∏–∑-–∑–∞ —Å–º–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ webcam_running –æ—Å—Ç–∞—ë—Ç—Å—è True
                    # –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ
                    if not st.session_state.get('webcam_running', True):
                        if st.session_state.webcam_detector:
                            if hasattr(st.session_state.webcam_detector.face_detector, 'close'):
                                st.session_state.webcam_detector.face_detector.close()
                            if hasattr(st.session_state.webcam_detector.emotion_recognizer, 'reset'):
                                st.session_state.webcam_detector.emotion_recognizer.reset()
                        st.session_state.webcam_detector = None
                        st.session_state.prev_webcam_params = None

                        # —Å–æ–æ–±—â–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ empty() —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ MediaFileStorageError
                        webcam_placeholder.info("üì∑ –í–µ–±-–∫–∞–º–µ—Ä–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                        emotions_placeholder.empty()
                        stats_placeholder.empty()
                        fps_placeholder.empty()

    with col2:
        # –°—Ç–∞—Ç—É—Å –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        st.markdown("#### –°—Ç–∞—Ç—É—Å")
        if st.session_state.get('webcam_running', False):
            st.success("‚úÖ –í–µ–±-–∫–∞–º–µ—Ä–∞ –∞–∫—Ç–∏–≤–Ω–∞")
            st.info("–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü –∏ —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        else:
            st.info("üì∑ –í–µ–±-–∫–∞–º–µ—Ä–∞ –≥–æ—Ç–æ–≤–∞")

        st.markdown("---")

        # –¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        st.markdown("#### ‚öôÔ∏è –¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        for key, value in st.session_state.backend_params.items():
            if key not in ['flip_h', 'show_preview']:
                st.metric(key.replace('_', ' ').title(), f"{value:.2f}" if isinstance(value, float) else value)

        st.markdown("---")

        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        st.markdown("#### üìù –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")
        st.markdown("""
        1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ–±-–∫–∞–º–µ—Ä—É
        2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –∫–∞–º–µ—Ä—É
        3. –≠–º–æ—Ü–∏–∏ –±—É–¥—É—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
        5. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ
        """)


# ============================================
# –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–°
# ============================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    display_header()
    display_sidebar()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—ç–∫–µ–Ω–¥–∞

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –≤–∫–ª–∞–¥–∫–∞—Ö
    tab1, tab2, tab3 = st.tabs(["üé¨ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞", "üì∑ –í–µ–±-–∫–∞–º–µ—Ä–∞", "‚ùì –°–ø—Ä–∞–≤–∫–∞"])

    with tab1:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if st.session_state.processing_status in ["idle", "starting"]:
            create_upload_section()

        if st.session_state.processing_status == "processing":
            process_video()
        elif st.session_state.processing_status == "completed":
            display_result()
        elif st.session_state.processing_status == "failed":
            display_error()

    with tab2:
        create_webcam_section()

    with tab3:
        st.markdown("### ‚ùì –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã")

        faqs = [
            {
                "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏?",
                "answer": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Å FaceAnalysisPipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –ª–∏—Ü –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π. –û–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –∫–∞–¥—Ä –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Ä–∏—Å—É—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–µ —Ä–∞–º–∫–∏ –∏ –º–µ—Ç–∫–∏ —ç–º–æ—Ü–∏–π."
            },
            {
                "question": "–ö–∞–∫–∏–µ —ç–º–æ—Ü–∏–∏ –º–æ–∂–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å?",
                "answer": "–°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ —ç–º–æ—Ü–∏–∏: —Ä–∞–¥–æ—Å—Ç—å, –≥—Ä—É—Å—Ç—å, –∑–ª–æ—Å—Ç—å, —É–¥–∏–≤–ª–µ–Ω–∏–µ, —Å—Ç—Ä–∞—Ö, –æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å –∏ –≤–æ–∑–º–æ–∂–Ω–æ –¥—Ä—É–≥–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏."
            },
            {
                "question": "–°–æ—Ö—Ä–∞–Ω—è–µ—Ç–µ –ª–∏ –≤—ã –º–æ–∏ –≤–∏–¥–µ–æ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è?",
                "answer": "–ù–µ—Ç. –í—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ. –í–∏–¥–µ–æ –≤—Ä–µ–º–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —É–¥–∞–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ."
            },
            {
                "question": "–ú–æ–≥—É –ª–∏ —è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–µ–∫—Ü–∏–∏?",
                "answer": "–î–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–∞–∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏, —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –∏ –ø–æ—Ä–æ–≥–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏."
            },
            {
                "question": "–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ –ª–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç—Å—è?",
                "answer": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏' –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏. –¢–∞–∫–∂–µ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ª–∏—Ü–∞ —Ö–æ—Ä–æ—à–æ –≤–∏–¥–Ω—ã –∏ –æ—Å–≤–µ—â–µ–Ω—ã."
            },
            {
                "question": "–î–ª—è —á–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è FPS?",
                "answer": "FPS (–∫–∞–¥—Ä—ã –≤ —Å–µ–∫—É–Ω–¥—É) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π FPS –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ—á–Ω–µ–µ."
            }
        ]

        for faq in faqs:
            with st.expander(f"**–í:** {faq['question']}"):
                st.markdown(f"**–û:** {faq['answer']}")

        st.markdown("---")

        st.markdown("### üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫")

        issues = [
            ("–í–µ–±-–∫–∞–º–µ—Ä–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
             "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞–º–µ—Ä–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É."),
            ("–õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç—Å—è", "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ —Ö–æ—Ä–æ—à–µ–º –æ—Å–≤–µ—â–µ–Ω–∏–∏."),
            ("–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞", "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤–∏–¥–µ–æ –∏–ª–∏ —á–∞—Å—Ç–æ—Ç—É –∫–∞–¥—Ä–æ–≤."),
            ("–û—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞",
             "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª face_detection.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."),
            ("–ù–∏–∑–∫–∏–π FPS", "–ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω–æ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä —Å GPU."),
        ]

        for issue, solution in issues:
            st.markdown(f"**{issue}:** {solution}")


# ============================================
# –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ============================================

if __name__ == "__main__":
    try:
        main()

        # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        atexit.register(lambda: st.session_state.get('detection_processor', EmotionDetectionProcessor()).reset())

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {str(e)}")
        st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

        if st.button("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"):
            st.rerun()
